{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-6e447082bc14>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/yijing/PycharmProjects/twitter/venv/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/yijing/PycharmProjects/twitter/venv/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/yijing/PycharmProjects/twitter/venv/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/yijing/PycharmProjects/twitter/venv/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/yijing/PycharmProjects/twitter/venv/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-1-6e447082bc14>:94: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch: 1 cost= 0.658663583\n",
      "Epoch: 2 cost= 0.081308805\n",
      "Epoch: 3 cost= 0.062135621\n",
      "Epoch: 4 cost= 0.056739183\n",
      "Epoch: 5 cost= 0.056844348\n",
      "Epoch: 6 cost= 0.052276557\n",
      "Epoch: 7 cost= 0.049678395\n",
      "Epoch: 8 cost= 0.045961984\n",
      "Epoch: 9 cost= 0.049054619\n",
      "Epoch: 10 cost= 0.050438156\n",
      "Epoch: 11 cost= 0.044954680\n",
      "Epoch: 12 cost= 0.048860191\n",
      "Epoch: 13 cost= 0.044961159\n",
      "Epoch: 14 cost= 0.043205635\n",
      "Epoch: 15 cost= 0.043237145\n",
      "Epoch: 16 cost= 0.049189926\n",
      "Epoch: 17 cost= 0.047260475\n",
      "Epoch: 18 cost= 0.040771322\n",
      "Epoch: 19 cost= 0.044779640\n",
      "Epoch: 20 cost= 0.043986418\n",
      "Epoch: 21 cost= 0.039995616\n",
      "Epoch: 22 cost= 0.041013842\n",
      "Epoch: 23 cost= 0.037607217\n",
      "Epoch: 24 cost= 0.038665009\n",
      "Epoch: 25 cost= 0.040346206\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9766\n",
      "Run the command line:\n",
      "--> tensorboard --logdir=/tmp/tensorflow_logs \n",
      "Then open http://0.0.0.0:6006/ into your web browser\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "# Using local folder MNIST_data after reading it the first time\n",
    "# mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "# tf Graph Input\n",
    "# mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "# 0-9 digits recognition => 10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "def conv2d(x, W, b):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "def max_pool_2x2(x):\n",
    "    # max_pool_2x2 downsamples a feature map by 2X\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "with tf.name_scope('reshape'):\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "# First convolutional layer - maps one grayscale image to 32 feature maps.\n",
    "with tf.name_scope('conv11'):\n",
    "    W_conv1 = tf.Variable(tf.truncated_normal([5,5,1,32], stddev=0.1))\n",
    "    b_conv1 = bias_variable([32])\n",
    "    h_conv1 = conv2d(x_image, W_conv1, b_conv1)\n",
    "#     h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "# Pooling layer - downsamples by 2X.\n",
    "with tf.name_scope('pooling1'):\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# Second convolutional layer -- maps 32 feature maps to 64.\n",
    "with tf.name_scope('conv12'):\n",
    "#     W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "    W_conv2 = tf.Variable(tf.truncated_normal([5,5,32,64], stddev=0.1))\n",
    "#     W_conv2 = tf.Variable(tf.zeros([5, 5, 32, 64]), name='Weights')\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = conv2d(h_pool1, W_conv2, b_conv2)\n",
    "#     h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "# Second pooling layer.\n",
    "with tf.name_scope('pooling2'):\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n",
    "# is down to 7x7x64 feature maps -- maps this to 1024 features.\n",
    "with tf.name_scope('fc1'):\n",
    "#     W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "#     W_fc1 = tf.Variable(tf.zeros([7 * 7 * 64, 1024]), name='Weights')\n",
    "    W_fc1 = tf.Variable(tf.truncated_normal([7*7*64,1024], stddev=0.1), name=\"w1\")\n",
    "    b_fc1 = bias_variable([1024])\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "    h_fc1 = tf.add(tf.matmul(h_pool2_flat, W_fc1), b_fc1)\n",
    "#     h_fc1 = tf.nn.softmax(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    h_fc1 = tf.nn.relu(h_fc1)\n",
    "\n",
    "# Dropout - controls the complexity of the model, prevents co-adaptation of\n",
    "# features.\n",
    "with tf.name_scope('dropout'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# Map the 1024 features to 10 classes, one for each digit\n",
    "with tf.name_scope('fc2'):\n",
    "#     W_fc2 = weight_variable([1024, 10])\n",
    "    W_fc2 = tf.Variable(tf.truncated_normal([1024,10], stddev=0.1), name=\"w2\")\n",
    "#     W_fc2 = tf.Variable(tf.zeros([1024, 10]), name='Weights')\n",
    "    b_fc2 = bias_variable([10])\n",
    "#     pred = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "    pred_logits = tf.add(tf.matmul(h_fc1_drop, W_fc2), b_fc2)\n",
    "    pred = tf.nn.softmax(pred_logits)\n",
    "#     pred = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "#     pred = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "# with tf.name_scope('Model'):\n",
    "# # Model\n",
    "#     pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "with tf.name_scope('Loss'):\n",
    "# Minimize error using cross entropy\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred_logits))\n",
    "#     cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "#     cost = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(labels=y, logits=pred))\n",
    "with tf.name_scope('AdamOptimizer'):\n",
    "# Gradient Descent\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "# Accuracy\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"accuracy\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "# Run the initializer\n",
    "    sess.run(init)\n",
    "# op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter('tmp/tensorflow_logs/cnn/train', graph=tf.get_default_graph())\n",
    "    summary_writer_test = tf.summary.FileWriter('tmp/tensorflow_logs/cnn/test')\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            step = epoch * total_batch + i\n",
    "            if step%10==0:\n",
    "                test_x, test_y = mnist.test.next_batch(batch_size)\n",
    "                # Get Test Summary for one batch and add summary to TensorBoard\n",
    "                summary = sess.run(merged_summary_op, \n",
    "                                   feed_dict={x: test_x, y: test_y, keep_prob: 1.0})\n",
    "                summary_writer_test.add_summary(summary, step)\n",
    "            else:\n",
    "                batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "                sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys,\n",
    "                                               keep_prob: 0.75})\n",
    "                # Get Train Summary for one batch and add summary to TensorBoard, and train\n",
    "                c, summary = sess.run([cost, merged_summary_op],\n",
    "                                      feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "                summary_writer.add_summary(summary, step)\n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "#         if (epoch+1) % display_epoch == 0:\n",
    "        print(\"Epoch:\", '%d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "#         print(sess.run(pred)[0])\n",
    "    print(\"Optimization Finished!\")\n",
    "    # Test model\n",
    "    # Calculate accuracy\n",
    "    print(\"Accuracy:\", acc.eval({x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0}))\n",
    "    print(\"Run the command line:\\n\" \\\n",
    "    \"--> tensorboard --logdir=/tmp/tensorflow_logs \" \\\n",
    "    \"\\nThen open http://0.0.0.0:6006/ into your web browser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
